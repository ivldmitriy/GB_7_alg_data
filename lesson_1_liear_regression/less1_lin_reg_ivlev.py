# -*- coding: utf-8 -*-
__author__ = "Dmitriy Ivlev"

"""less1_lin_reg_ivlev.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PQthI0ggDxSU9Cv8wDx-v8_ctc0CubpO

##Подготовительные процедуры <a class="anchor" id="preparation"></a><center>

__Задача:__ предсказание баллов ЕГЭ ученика в зависимости от кол-ва лет стажа его репетитора

####**Библиотеки**
"""

import numpy as np

"""####**Признаки и целевые значения**"""

X = np.array([[ 1,  1],
              [ 1,  1],
              [ 1,  2],
              [ 1,  5],
              [ 1,  3],
              [ 1,  0],
              [ 1,  5],
              [ 1, 10],
              [ 1,  1],
              [ 1,  2]])

y = [45, 55, 50, 55, 60, 35, 75, 80, 50, 60]

"""####**Функции**"""

def calc_mae(y, y_pred):
    err = np.mean(np.abs(y - y_pred))
    return err

def calc_mse(y, y_pred):
    err = np.mean((y - y_pred)**2)
    return err

"""## **Домашнее задание** <a class="anchor" id="hw"></a><center>

###1. Подберите скорость обучения (eta) и количество итераций

Расчитаем аналитические веса и MSE
"""

W_analytical = np.linalg.inv(np.dot(X.T, X)) @ X.T @ y
y_pred_analytical = W_analytical[0] * X[:, 0] + W_analytical[1] * X[:, 1]
mse_analytical = calc_mse(y, y_pred_analytical)
print(f'Analytical weights are {W_analytical}')
print(f'Analytical MSE is {mse_analytical:.2f}')

"""Оформим перебор с помощью цикла"""

#Перебор значений занимает довольно значительное время
'''
n = X.shape[0]

# Создал наборы числа итераций и скорости обучения для перебора
eta_diapason = np.linspace(0, 2, 1000)
iter_diapason = range(0, 500, 25)
#eta = 1e-2 
#n_iter = 100

# Создал заготовку для хранения результата
res = []
err = np.inf

# Начинаю перебирать
for eta in eta_diapason:
    for n_iter in iter_diapason:
        # заготовка под новую строку в массиве результата
        row = [eta, n_iter]
        W = np.array([1, 0.5])
        # Отказываюсь от вывода на экран
        #print(f'Number of objects = {n} \
        #      \nLearning rate = {eta} \
        #      \nInitial weights = {W} \n')

        for i in range(n_iter):
            y_pred = np.dot(X, W)
            err = calc_mse(y, y_pred)
            for k in range(W.shape[0]):
                W[k] -= eta * (1/n * 2 * X[:, k] @ (y_pred - y))
            if i % 10 == 0:
                eta /= 1.1
        
        row.append(err)
        res.append(row)
                # Отказываюсь от вывода на экран
                #print(f'Iteration #{i}: W_new = {W}, MSE = {round(err, 2)}')
'''

'''
data = np.array(res)
data = data[np.argsort(data[:, 2])]
print(f'Iteration number {data[0, 1]:.2f}: eta = {data[0, 0]}, MSE = {data[0, 2]:.2f}')
print(f'Analytical MSE is {mse_analytical:.2f}')
'''

"""Подбор параметров дал наилучший результат для 125 итераций со скоростью обучения 0.1085

Любопытно, что эта скорость условно "большая", ошибка первоначально нарастает, наблюдается разлет MSE. И только на больших итерациях ошибка снова уменьшается, а значения весов приближаются к аналитическим.
"""

n = X.shape[0]

#eta = 1e-2 
#n_iter = 100
eta = 0.1085  # NEW
n_iter = 125  # NEW

W = np.array([1, 0.5])
print(f'Number of objects = {n} \
       \nLearning rate = {eta} \
       \nInitial weights = {W} \n')

for i in range(n_iter):
    y_pred = np.dot(X, W)
    err = calc_mse(y, y_pred)
    for k in range(W.shape[0]):
        W[k] -= eta * (1/n * 2 * X[:, k] @ (y_pred - y))
    if i % 10 == 0:
        eta /= 1.1
        print(f'Iteration #{i}: W_new = {W}, MSE = {round(err, 2)}')

print(f'\nCalculated weights are {W}')  # NEW
print(f'Analytical weights are {W_analytical}')  # NEW
print(f'\nCalculated MSE is {err:.5f}')  # NEW
print(f'Analytical MSE is {mse_analytical}')  # NEW

"""###2*. В этом коде мы избавляемся от итераций по весам, но тут есть ошибка, исправьте ее"""

n = X.shape[0]

eta = 1e-2 
n_iter = 100

W = np.array([1, 0.5])
print(f'Number of objects = {n} \
       \nLearning rate = {eta} \
       \nInitial weights = {W} \n')

for i in range(n_iter):
    y_pred = np.dot(X, W)
    err = calc_mse(y, y_pred)
#     for k in range(W.shape[0]):
#         W[k] -= eta * (1/n * 2 * X[:, k] @ (y_pred - y))
    #W -= eta * (1/n * 2 * np.dot(X, y_pred - y))
    # Правила матричного умножения, строка на столбец, в исходном варианте срез - строка
    W -= eta * (1/n * 2 * np.dot(X.T, y_pred - y)) # NEW
    if i % 10 == 0:
        print(f'Iteration #{i}: W_new = {W}, MSE = {round(err,2)}')



"""###3*. Вместо того, чтобы задавать количество итераций, задайте другое условие останова алгоритма - когда веса перестают изменяться меньше определенного порога  ϵ ."""

n = X.shape[0]

eta = 1e-2 
#n_iter = 100
eps = 1e-2  # NEW
i = 0 # NEW
weight_dist = np.inf  # NEW

W = np.array([1, 0.5])
print(f'Number of objects = {n} \
       \nLearning rate = {eta} \
       \nInitial weights = {W} \n')

#for i in range(n_iter):
while weight_dist > eps:  # NEW
    y_pred = np.dot(X, W)
    err = calc_mse(y, y_pred)
    W_old = W.copy()  # NEW
    #for k in range(W.shape[0]):
        #W[k] -= eta * (1/n * 2 * X[:, k] @ (y_pred - y))
    W -= eta * (1/n * 2 * np.dot(X.T, y_pred - y)) # NEW
    weight_dist = np.linalg.norm(W - W_old, ord=2)  # NEW
    # порядок 2 - наибольшая из разниц по всем весам
    if i % 10 == 0:
        eta /= 1.1
        print(f'Iteration #{i}: W_new = {W}, MSE = {round(err, 2)}')
    i += 1  # NEW

